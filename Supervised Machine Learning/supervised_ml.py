# -*- coding: utf-8 -*-
"""module6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iIApM82NK-D7dzFAn3V9Gg_w5I5hFtzL
"""

#    TASK1   #



import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt

from sklearn.preprocessing import OneHotEncoder, LabelEncoder,PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.svm import SVR

from sklearn.metrics import mean_squared_error, r2_score

from sklearn.decomposition import PCA

data=pd.read_excel("/content/house_club.xlsx")
data.head()

data.info()

data.describe()

data.columns

data.shape

data.isnull().sum()

# Price distribution
plt.figure(figsize=(8,5))
sns.histplot(data['Price'], bins=50, kde=True)
plt.title('Price Distribution')
plt.xlabel('Price')
plt.ylabel('Count')
plt.show()

# Area distribution
plt.figure(figsize=(8,5))
sns.histplot(data['Area'], bins=50, kde=True, color='orange')
plt.title('Area Distribution')
plt.xlabel('Area (sq ft)')
plt.ylabel('Count')
plt.show()

# Price vs Area scatter
plt.figure(figsize=(8,5))
sns.scatterplot(x='Area', y='Price', data=data, alpha=0.4)
plt.title('Price vs Area')
plt.xlabel('Area (sq ft)')
plt.ylabel('Price')
plt.show()

plt.figure(figsize=(10,6))
city_price = data.groupby('City')['Price'].mean().sort_values(ascending=False)
sns.barplot(x=city_price.index, y=city_price.values)
plt.title('Average Price by City')
plt.ylabel('Average Price')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(8,5))
sns.countplot(x='No. of Bedrooms', data=data, palette='Set2')
plt.title('Bedroom Count Distribution')
plt.xlabel('Number of Bedrooms')
plt.ylabel('Number of Properties')
plt.show()

#2. Build a Simple Linear Regression model to predict the Sale price of the house. Use Area as the independent variable

df1=data

X = df1[['Area']]

Y = df1[['Price']]

X

Y

Xtrain, Xtest, Ytrian, Ytest = train_test_split(X,Y,random_state=42,test_size=0.2)

liner = LinearRegression()
liner_model = liner.fit(Xtrain,Ytrian)

def result(model):
  Y_pre = model.predict(Xtest)
  mse = mean_squared_error(Ytest,Y_pre)
  r2 = r2_score(Ytest, Y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)





#3. Build Multiple Linear Regression model to predict Sale price of the house

df2=data

lcoder= LabelEncoder()
df2['City_label'] = lcoder.fit_transform(df2['City'])
df2['Location_label'] = lcoder.fit_transform(df2['Location'])

df2

X = df2.drop(['City','Location','Price'],axis=1)
Y = df2['Price']

X

Y

scaler_s = StandardScaler()
X_scaled_s = scaler_s.fit_transform(X)

Xtrain, Xtest, Ytrian, Ytest = train_test_split(X,Y,random_state=42,test_size=0.2)

liner = LinearRegression()
liner_model = liner.fit(Xtrain,Ytrian)

def result(model):
  Y_pre = model.predict(Xtest)
  mse = mean_squared_error(Ytest,Y_pre)
  r2 = r2_score(Ytest, Y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)



#4. Use dimensionality reduction technique PCA/LDA and build Multiple Linear
#Regression model to predict Sale price of the house.

df3=data

lcoder= LabelEncoder()
df3['City_label'] = lcoder.fit_transform(df3['City'])
df3['Location_label'] = lcoder.fit_transform(df3['Location'])

X = df3.drop(['City','Location','Price'],axis=1)
Y = df3['Price']

Xtrain, Xtest, Ytrian, Ytest = train_test_split(X,Y,random_state=42,test_size=0.2)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(Xtrain)
X_test_scaled = scaler.transform(Xtest)

pca = PCA(n_components=3) #Or PCA(0.95)
X_train_pca = pca.fit_transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

print("PCA Explained Variance Ratio:", pca.explained_variance_ratio_)

liner = LinearRegression()
liner_model = liner.fit(Xtrain,Ytrian)

def result(model):
  Y_pre = model.predict(Xtest)
  mse = mean_squared_error(Ytest,Y_pre)
  r2 = r2_score(Ytest, Y_pre)
  return mse,r2

models = [liner_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)





#5. Build a model using Lasso and Ridge regression to reduce model complexity.

df4=data

lcoder= LabelEncoder()
df4['City_label'] = lcoder.fit_transform(df4['City'])
df4['Location_label'] = lcoder.fit_transform(df4['Location'])

df4

X = df4.drop(['City','Location','Price'],axis=1)
Y = df4['Price']

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(Xtrain)
X_test_scaled = scaler.transform(Xtest)

Xtrain, Xtest, Ytrian, Ytest = train_test_split(X,Y,random_state=42,test_size=0.2)

ridge = Ridge(alpha=1.0)
ridge_model = ridge.fit(Xtrain, Ytrian)

lasso = Lasso(alpha=0.1)
lasso_model= lasso.fit(Xtrain, Ytrian)

#6. Build an SVR model to predict Sale price of the house.

SVR = SVR()
SVR_model = SVR.fit(Xtrain,Ytrian)

#7. Build Decision Tree Regressor to predict Sale price of the house.

DecisionTree = DecisionTreeRegressor()
DT_model = DecisionTree.fit(Xtrain,Ytrian)

#8. Build Random Forest Regression model to predict Sale price of the house.

RandomForest = RandomForestRegressor()
RF_model = RandomForest.fit(Xtrain,Ytrian)

def result(model):
  Y_pre = model.predict(Xtest)
  mse = mean_squared_error(Ytest,Y_pre)
  r2 = r2_score(Ytest, Y_pre)
  return mse,r2

models = [ridge_model, lasso_model, SVR_model, DT_model, RF_model]
for i in models:
  mse, r2 = result(i)
  print("\tmodel: ",i)
  print("MSE: ",mse)
  print("R2: ",r2)

#9.Use GridsearchCV and RandomizedsearchCV for tuning hyperparameters andfit your model on the optimal.

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

RandomForest = RandomForestRegressor()
RF_model = RandomForest.fit(Xtrain,Ytrian)

params = {
    'bootstrap': [True],
    'max_depth': [80, 100],
    'min_samples_split': [8, 12],
    'n_estimators': [100, 300]
}

from sklearn.model_selection import StratifiedKFold

cv_object = StratifiedKFold(n_splits = 2)

grid_search = GridSearchCV(estimator = RF_model, param_grid = params, cv = cv_object, verbose = 0, return_train_score = True)
grid_search.fit(X_train_scaled, Ytrian.ravel())



param_dist = {
    'n_estimators': np.arange(100, 1000, 100),      # number of trees
    'max_depth': [None, 5, 10, 20, 30],             # depth of each tree
    'min_samples_split': [2, 5, 10],                # split threshold
    'min_samples_leaf': [1, 2, 4],                  # leaf threshold
    'max_features': ['sqrt'],                           # feature sampling
    'bootstrap': [True, False]                      # sampling mode
}

random_search = RandomizedSearchCV(
    estimator=RF_model,
    param_distributions=param_dist,
    n_iter=10,              # number of random combinations
    cv=3,                   # 5-fold cross-validation
    verbose=2,
    n_jobs=1,
    random_state=42,             # use all CPU cores
)

RF_model=random_search.fit(Xtrain, Ytrian)

RF_model















#10.Model Selection: Evaluate and compare performance of all the models tofindthebest model.

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

models = {
    'ridge': ridge_model,
    'lasso': lasso_model,
    'SVR': SVR_model,
    'DecisionTree': DT_model,
    'RandomForest': RF_model,
}

results = []

for name, model in models.items():
    y_pred = model.predict(X_test_scaled)
    mse = mean_squared_error(Ytest, y_pred)
    mae = mean_absolute_error(Ytest, y_pred)
    r2 = r2_score(Ytest, y_pred)

    results.append({
        'Model': name,
        'MSE': mse,
        'MAE': mae,
        'R2 Score': r2
    })

import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.sort_values(by='R2 Score', ascending=False))









#    TASK2   #



import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder, LabelEncoder,PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report

data=pd.read_csv("/content/BankChurners.csv")
data.head()

data.info()

data.describe()

data.shape

data.columns

data.isnull().sum()

sns.countplot(x='Attrition_Flag', data=data)
plt.title('Distribution of Target Variable')
plt.show()

# Education Level Distribution
plt.subplot()
sns.countplot(data=data, y='Education_Level',
              order=data['Education_Level'].value_counts().index,
              palette='magma')
plt.title('Education Level Distribution')

#Age Distribution
plt.subplot()
sns.histplot(data['Customer_Age'], bins=20, kde=True, color='teal')
plt.title('Customer Age Distribution')

numeric_data = data.select_dtypes(include=['int64', 'float64'])
plt.figure(figsize=(12,8))
sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

sns.pairplot(data, hue='Attrition_Flag', vars=['Total_Trans_Amt','Total_Trans_Ct','Avg_Open_To_Buy'])
plt.show()

columns=['Gender','Education_Level','Marital_Status','Income_Category','Card_Category','Attrition_Flag']
for i in columns:
  lcoder=LabelEncoder()
  data[f"{i}_label"]=lcoder.fit_transform(data[i])

data

X = data.drop(['Gender','Education_Level','Marital_Status','Income_Category','Card_Category','Attrition_Flag','Attrition_Flag_label'],axis=1)
Y = data['Attrition_Flag_label']

scaler_s = StandardScaler()
X_scaled_s = scaler_s.fit_transform(X)

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X,Y,random_state=42,test_size=0.2)

Logistic_Regression = LogisticRegression()
LR_model = Logistic_Regression.fit(Xtrain,Ytrain)

Naive_Bayes = GaussianNB()
NB_model = Naive_Bayes.fit(Xtrain,Ytrain)

KNN = KNeighborsClassifier()
Knn_model = KNN.fit(Xtrain,Ytrain)

SVM = SVC()

#SVM_model = SVM.fit(Xtrain,Ytrain)

param_grid = {
    "C": [0.1, 1, 10],
    "gamma": ["scale", "auto"]
}

from sklearn.svm import SVC

grid = GridSearchCV(
    estimator=SVM,
    param_grid=param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

SVM_model=grid.fit(Xtrain, Ytrain)

SVM_model

print("Best Parameters:")
print(grid.best_params_)

best_svc = grid.best_estimator_

Ypred = best_svc.predict(Xtest)

print("\n Accuracy:", accuracy_score(Ytest, Ypred))
print("\n Classification Report:\n", classification_report(Ytest, Ypred))

Decision_Tree = DecisionTreeClassifier()
#DT_model = Decision_Tree.fit(Xtrain,Ytrain)

param_grid = {
    'max_depth': [3, 5, 10, None],
    'criterion': ['gini', 'entropy'],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid = GridSearchCV(
    estimator=Decision_Tree,
    param_grid=param_grid,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid = GridSearchCV(Decision_Tree, param_grid, cv=5)
DT_model=grid.fit(Xtrain, Ytrain)

DT_model

print("Best Parameters:")
print(grid.best_params_)

best_dt = grid.best_estimator_

Ypred = best_dt.predict(Xtest)

print("\n Accuracy:", accuracy_score(Ytest, Ypred))
print("\n Classification Report:\n", classification_report(Ytest, Ypred))

Random_Forest = RandomForestClassifier()
#RF_model = Random_Forest.fit(Xtrain,Ytrain)

param_dist = {
    'n_estimators': np.arange(100, 1000, 100),      # number of trees
    'max_depth': [None, 5, 10, 20, 30],             # depth of each tree
    'min_samples_split': [2, 5, 10],                # split threshold
    'min_samples_leaf': [1, 2, 4],                  # leaf threshold
    'max_features': ['sqrt'],                           # feature sampling
    'bootstrap': [True, False]                      # sampling mode
}


#'max_features': ['auto', 'sqrt', 'log2']

random_search = RandomizedSearchCV(
    estimator=Random_Forest,
    param_distributions=param_dist,
    n_iter=20,              # number of random combinations
    cv=5,                   # 5-fold cross-validation
    verbose=2,
    random_state=42,
    n_jobs=-1               # use all CPU cores
)

RF_model=random_search.fit(Xtrain, Ytrain)

RF_model

print("Best Parameters Found:")
print(random_search.best_params_)

best_rf = random_search.best_estimator_

y_pred = best_rf.predict(Xtest)

print("\n Accuracy:", accuracy_score(Ytest, y_pred))
print("\n Classification Report:\n")
print(classification_report(Ytest, y_pred))



def result(model):
  y_pre = model.predict(Xtest)
  acc = accuracy_score(Ytest, y_pre)
  prec = precision_score(Ytest, y_pre)
  rec = recall_score(Ytest, y_pre)
  f1 = f1_score(Ytest, y_pre)
  return acc, prec, rec, f1



models = [LR_model,NB_model,Knn_model,SVM_model ,DT_model,RF_model]
for model in models:
  acc, prec, rec, f1 = result(model)
  print("\tmodel: ",model)
  print("Accuracy: ",acc)
  print("Precision: ",prec)
  print("Recall: ",rec)
  print("F1: ",f1)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

models = {
    'Logistic_Regression': LR_model,
    'Naive_Bayes': NB_model,
    'KNN': Knn_model,
    'SVM': SVM_model ,
    'Decision_Tree': DT_model,
    'Random_Forest': RF_model,
}

results = []

for name, model in models.items():
    y_pre = model.predict(Xtest)
    acc = accuracy_score(Ytest, y_pre)
    prec = precision_score(Ytest, y_pre)
    rec = recall_score(Ytest, y_pre)
    f1 = f1_score(Ytest, y_pre)


    results.append({
        'Model': name,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1': f1
    })

import pandas as pd
results_df = pd.DataFrame(results)
print(results_df.sort_values(by='F1', ascending=False))

