# -*- coding: utf-8 -*-
"""module8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EkMp4g2ktdi74TvVNHxQ-lrvJqynGbeh
"""

#TASK1: Predicting Customer Churn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.datasets
from sklearn.model_selection import train_test_split

data=pd.read_csv("/content/BankChurners.csv")
data.head()

data.tail()

data.shape

data.info()

data.isnull().sum()

data.columns

data.describe

from sklearn.preprocessing import LabelEncoder
lcoder= LabelEncoder()
data['CLIENTNUM_label'] = lcoder.fit_transform(data['CLIENTNUM'])
data['Attrition_Flag_label'] = lcoder.fit_transform(data['Attrition_Flag'])
data['Education_Level_label'] = lcoder.fit_transform(data['Education_Level'])
data['Marital_Status_label'] = lcoder.fit_transform(data['Marital_Status'])
data['Income_Category_label'] = lcoder.fit_transform(data['Income_Category'])
data['Card_Category_label'] = lcoder.fit_transform(data['Card_Category'])
data['Gender_label'] = lcoder.fit_transform(data['Gender'])

data

data['Attrition_Flag_label'].value_counts()

data.info()

data.groupby('Attrition_Flag_label').mean()



X = data.drop(['CLIENTNUM','Attrition_Flag','Education_Level','Marital_Status','Income_Category','Card_Category','Gender','Attrition_Flag_label'],axis=1)
Y = data['Attrition_Flag_label']

X

Y

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=5)

print(X.shape, X_train.shape, X_test.shape)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_std = scaler.fit_transform(X_train)

X_test_std = scaler.transform(X_test)

print(X_train_std.shape)
print(X_test_std.shape)

# importing tensorflow and Keras
import tensorflow as tf
tf.random.set_seed(3)
from tensorflow import keras

model = keras.Sequential([
                          keras.layers.Flatten(input_shape=(22,)),
                          keras.layers.Dense(25, activation='relu'),
                          #keras.layers.Dense(10, activation='relu'),
                          keras.layers.Dense(2, activation='sigmoid')
])



model = Sequential()

model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dropout(0.3))

model.add(Dense(32, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(1, activation='sigmoid'))



model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(X_train_std, Y_train, validation_split=0.1, epochs=10)

y_pred = (model.predict(X_test) > 0.5).astype(int)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'lower right')

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')

plt.legend(['training data', 'validation data'], loc = 'upper right')

loss, accuracy = model.evaluate(X_test_std, Y_test)
print(accuracy)

print(X_test_std.shape)
print(X_test_std[0])

Y_pred = model.predict(X_test_std)

print(Y_pred.shape)
print(Y_pred[0])

#  argmax function
my_list = [0.25, 0.56]

index_of_max_value = np.argmax(my_list)
print(my_list)
print(index_of_max_value)

# converting the prediction probability to class labels
Y_pred_labels = [np.argmax(i) for i in Y_pred]
print(Y_pred_labels)























#TASK2: Digit Recognizer

import tensorflow
from tensorflow import keras
from keras.layers import Dense,Conv2D,Flatten
from keras import Sequential
from keras.datasets import mnist
import matplotlib.pyplot as plt
import numpy as np
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train = X_train.reshape(-1, 28, 28, 1) / 255.0
X_test  = X_test.reshape(-1, 28, 28, 1) / 255.0

model = Sequential()

model.add(Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D(2,2))

model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(MaxPooling2D(2,2))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(10, activation='softmax'))

model.summary()

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.2
)

test_loss, test_accuracy = model.evaluate(X_test, y_test)
print("Test Accuracy:", test_accuracy)

predictions = model.predict(X_test)
i = np.random.randint(0, len(X_test))
print("Index used:", i)

plt.imshow(X_test[i].reshape(28,28), cmap='gray')
plt.title(f"Predicted: {np.argmax(predictions[i])}")
plt.axis('off')
plt.show()























#TASK3: Sentiment Analysis

import pandas as pd

imdb_data =pd.read_csv("/content/imdb-movies-dataset.csv")
imdb_data .head()

# Display the shape and head of the data
print("Shape of dataset:", imdb_data.shape)

# Remove rows where Review or Rating is missing
imdb_data = imdb_data[ (imdb_data['Review'].notnull()) & (imdb_data['Rating'].notnull()) ]

# Create a new column called 'Sentiment'
# If Rating is 7 or more â†’ Positive (1), else Negative (0)
sentiments = []
for rating in imdb_data['Rating']:
    if rating >= 7:
        sentiments.append(1)
    else:
        sentiments.append(0)

# Add the new column to the dataset
imdb_data['Sentiment'] = sentiments

# Separate features (X) and labels (y)
X = imdb_data['Review']
y = imdb_data['Sentiment']

# Check how many positive and negative reviews we have
print("Positive reviews:", y.sum())
print("Negative reviews:", len(y) - y.sum())

# Tokenize and Pad Sequences

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Limit vocabulary size to 5000 most common words
max_words = 5000
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X)

# Convert reviews to sequences of integers
sequences = tokenizer.texts_to_sequences(X)

# Pad sequences to make all inputs the same length
maxlen = 200
X_padded = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')

X_padded.shape

max_length = max([len(seq) for seq in sequences])
max_length

# Train-Test Split

from sklearn.model_selection import train_test_split
import numpy as np

# Convert target to numpy array
y = np.array(y)

# Split into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)

X_test.shape

# Build and Train RNN Model

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense

# Define model architecture
rnn_model = Sequential([
    Embedding(input_dim=5000, output_dim=32, input_length=200),
    SimpleRNN(units=64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Compile the model
rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
rnn_history = rnn_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

import matplotlib.pyplot as plt

# Accuracy plot
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(rnn_history.history['accuracy'], label='Training Accuracy')
plt.plot(rnn_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('RNN Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


# Loss plot
plt.subplot(1, 2, 2)
plt.plot(rnn_history.history['loss'], label='Training Loss')
plt.plot(rnn_history.history['val_loss'], label='Validation Loss')
plt.title('RNN Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()



plt.tight_layout()
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Define model architecture
lstm_model = Sequential([
    Embedding(input_dim=5000, output_dim=32, input_length=200),
    LSTM(units=64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Compile the model
lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
lstm_history = lstm_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

# Accuracy plot
plt.figure(figsize=(14, 5))
plt.subplot(1, 2, 1)
plt.plot(lstm_history.history['accuracy'], label='Training Accuracy')
plt.plot(lstm_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('LSTM Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


# Loss plot
plt.subplot(1, 2, 2)
plt.plot(lstm_history.history['loss'], label='Training Loss')
plt.plot(lstm_history.history['val_loss'], label='Validation Loss')
plt.title('LSTM Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()


plt.tight_layout()
plt.show()

from tensorflow.keras.layers import GRU

# Define GRU model
gru_model = Sequential([
    Embedding(input_dim=5000, output_dim=32, input_length=100),
    GRU(units=64, return_sequences=False),
    Dense(1, activation='sigmoid')
])

# Compile the model
gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
gru_history = gru_model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))

plt.figure(figsize=(14, 5))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(gru_history.history['accuracy'], label='Training Accuracy')
plt.plot(gru_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('GRU Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()


# Loss
plt.subplot(1, 2, 2)
plt.plot(gru_history.history['loss'], label='Training Loss')
plt.plot(gru_history.history['val_loss'], label='Validation Loss')
plt.title('GRU Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()


plt.tight_layout()
plt.show()



# Model Selection: Evaluate and compare performance of all the models to find the best model suited for this application.

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
y_pred = rnn_model.predict(X_test)
y_pred1 = []
for i in y_pred:
    a = np.round(i)
    y_pred1.append(a)
y_pred1
acc = accuracy_score(y_test, y_pred1)
print("Accuracy:", acc)
print("\nClassification Report:\n", classification_report(y_test, y_pred1))

y_pred =lstm_model.predict(X_test)
y_pred1 = []
for i in y_pred:
    a = np.round(i)
    y_pred1.append(a)
y_pred1
acc = accuracy_score(y_test, y_pred1)
print("Accuracy:", acc)
print("\nClassification Report:\n", classification_report(y_test, y_pred1))

y_pred = gru_model.predict(X_test)
y_pred1 = []
for i in y_pred:
    a = np.round(i)
    y_pred1.append(a)
y_pred1
acc = accuracy_score(y_test, y_pred1)
print("Accuracy:", acc)
print("\nClassification Report:\n", classification_report(y_test, y_pred1))

# Compare accuracy and loss across RNN, LSTM, GRU
plt.figure(figsize=(14, 6))

# Accuracy comparison
plt.subplot(1, 2, 1)
plt.plot(rnn_history.history['val_accuracy'], label='RNN')
plt.plot(lstm_history.history['val_accuracy'], label='LSTM')
plt.plot(gru_history.history['val_accuracy'], label='GRU')
plt.title('Validation Accuracy Comparison')
plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.legend()

# Loss comparison
plt.subplot(1, 2, 2)
plt.plot(rnn_history.history['val_loss'], label='RNN')
plt.plot(lstm_history.history['val_loss'], label='LSTM')
plt.plot(gru_history.history['val_loss'], label='GRU')
plt.title('Validation Loss Comparison')
plt.xlabel('Epochs')
plt.ylabel('Validation Loss')
plt.legend()

plt.tight_layout()
plt.show()





























#TASK4 : Build a text classifier model for emotion detection in text. [NLP]

import pandas as pd
import numpy as np
import re

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

data = pd.read_csv("/content/combined_emotion.csv")
data.head()

data.shape

data.info()

data.columns

data.isnull().sum()

print(f"emotion value count: \n{data['emotion'].value_counts()}")

#Bar plot to visualize the total counts of each rating

data['emotion'].value_counts().plot.bar(color = 'green')
plt.title('emotion distribution count')
plt.xlabel('emotion')
plt.ylabel('Count')
plt.show()

print(f"emotion value count - percentage distribution: \n{round(data['emotion'].value_counts()/data.shape[0]*100,2)}")

fig = plt.figure(figsize=(7,7))

colors = ('red', 'green', 'blue','orange','yellow','pink')

wp = {'linewidth':1, "edgecolor":'black'}

tags = data['emotion'].value_counts()/data.shape[0]

explode=(0.1,0.1,0.1,0.1,0.1,0.1)

tags.plot(kind='pie', autopct="%1.1f%%", shadow=True, colors=colors, startangle=90, wedgeprops=wp, explode=explode, label='Percentage wise distrubution of rating')

from io import  BytesIO

graph = BytesIO()

fig.savefig(graph, format="png")

def clean_text(text):
    text = text.lower()
    text = re.sub(r"http\S+", "", text)
    text = re.sub(r"[^a-z\s]", "", text)
    text = re.sub(r"\s+", " ", text)
    return text

data['clean_text'] = data['sentence'].apply(clean_text)

le = LabelEncoder()
data['emotion_encoded'] = le.fit_transform(data['emotion'])

print(le.classes_)

X = data['clean_text']
y = data['emotion_encoded']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

tfidf = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1,2),
    stop_words='english'
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d',
            xticklabels=le.classes_,
            yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

def predict_emotion(text):
    text = clean_text(text)
    vector = tfidf.transform([text])
    pred = model.predict(vector)
    return le.inverse_transform(pred)[0]

predict_emotion("i feel like a jerk because the library students who all claim to love scrabble cant be bothered to participate and clearly scrabble is an inappropriate choice for a group of students whose native language isnt english")































#Task 5: Text Classification (Average Word2vec) [NLP]

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

!pip install -U gensim

from gensim.models import Word2Vec

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

#Load Dataset
df = pd.read_csv("/content/combined_sentiment_data.csv")
df.head()

df['sentiment'].value_counts()

print(df.isnull().sum())

df = df.rename(columns={
    'sentence': 'text',
    'sentiment': 'sentiment'
})

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def preprocess(text):
    text = str(text).lower()
    text = re.sub(r"[^a-z\s]", "", text)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return tokens

df['tokens'] = df['text'].apply(preprocess)

df[['text', 'tokens']].head()

le = LabelEncoder()
df['sentiment_encoded'] = le.fit_transform(df['sentiment'])

w2v_model = Word2Vec(
    sentences=df['tokens'],
    vector_size=100,
    window=5,
    min_count=1,              # keeps rare words
    workers=4,
    sg=1                      # skip-gram (better here)
)

print("great" in w2v_model.wv)     # True
print("jawbone" in w2v_model.wv)   # True or False (both OK)
print(len(w2v_model.wv))

def average_word2vec(tokens, model, vector_size):
    vectors = [model.wv[w] for w in tokens if w in model.wv]
    if len(vectors) == 0:
        return np.zeros(vector_size)
    return np.mean(vectors, axis=0)

X = df['tokens']
y = df['sentiment_encoded']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train_vec = np.array([
    average_word2vec(tokens, w2v_model, 100) for tokens in X_train
])

X_test_vec = np.array([
    average_word2vec(tokens, w2v_model, 100) for tokens in X_test
])

model = LogisticRegression(max_iter=1000)
model.fit(X_train_vec, y_train)

y_pred = model.predict(X_test_vec)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names=le.classes_))

def predict_sentiment(review):
    tokens = preprocess(review)
    vec = average_word2vec(tokens, w2v_model, 100).reshape(1, -1)
    pred = model.predict(vec)
    return le.inverse_transform(pred)[0]

predict_sentiment("Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!")























#TASK5: Avengers Face Detection [CV]

# Import all necessary
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import cv2 as cv
import re
import requests

from sklearn.metrics import confusion_matrix,  multilabel_confusion_matrix
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import datasets, layers, models

import zipfile
import os

zip_path = "/content/archive (8).zip"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content/data")

import tensorflow as tf

img_size = (224, 224)
batch_size = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/data/cropped_images/",
    image_size=img_size,
    batch_size=batch_size,
    validation_split=0.2,
    subset="training",
    seed=42
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/data/cropped_images/",
    image_size=img_size,
    batch_size=batch_size,
    validation_split=0.2,
    subset="validation",
    seed=42
)

from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Rescaling(1./255, input_shape=(224, 224, 3)),
    layers.Conv2D(32, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(len(train_ds.class_names), activation='softmax')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=15
)

from google.colab import files
uploaded = files.upload()

import numpy as np
from tensorflow.keras.preprocessing import image

img_path = list(uploaded.keys())[0]
img = tf.keras.utils.load_img(img_path, target_size=(224, 224))
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0)

pred = model.predict(img_array)
class_names = train_ds.class_names

print("Prediction:", class_names[np.argmax(pred)])





